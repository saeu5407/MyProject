{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "############################################################\n",
    "# 데이콘 3월 스타크래프트 경기 승률 예측 프로젝트\n",
    "# dyddl1993@naver.com | https://github.com/saeu5407/\n",
    "############################################################\n",
    "############################################################\n",
    "\n",
    "# 사용 라이브러리\n",
    "# 시각화용 라이브러리는 노트북의 한계로 사용안함\n",
    "print(\"=============================\")\n",
    "print(\"import lib\")\n",
    "print(\"=============================\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from bayes_opt import BayesianOptimization\n",
    "from lightgbm import LGBMClassifier, plot_importance\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# 기본 전처리용 함수들\n",
    "# 각 플레이어의 행동을 카운트 한 변수들, 플레이어의 행동 차이를 계산한 변수\n",
    "# 각 플레이어의 종족\n",
    "############################################################\n",
    "# 전처리용 함수 1\n",
    "def make_feature(x, i, type='train'):\n",
    "    a = x.loc[x.game_id == i, :]\n",
    "    if type == 'test':\n",
    "        a1 = a.iloc[0:1, 0:1].reset_index(drop=True)  # game id(test는 winner존재x)\n",
    "    else:\n",
    "        a1 = a.iloc[0:1, 0:2].reset_index(drop=True)  # game id와 winner\n",
    "    a2 = a.apply('max', axis=0).time  # time의 max값\n",
    "    a3 = a.groupby(['player', 'species'])['time'].count().reset_index()  # 0번, 1번 플레이어의 종족(교차곱하면 좋을듯)\n",
    "    b = a.set_index(['game_id']).groupby(['player', 'event'])['time'].count().reset_index()\n",
    "    b1 = b.loc[b.player == 0, ['event', 'time']]\n",
    "    b2 = b.loc[b.player == 1, ['event', 'time']]\n",
    "    c = pd.merge(b1, b2, on='event', how='outer').fillna(0)\n",
    "    c = c.append({'event': 'ALL', 'time_x': c.apply('sum', axis=0).time_x, 'time_y': c.apply('sum', axis=0).time_y},\n",
    "                 ignore_index=True)\n",
    "    c['per_0'] = (c.time_x - c.time_y)\n",
    "    c = c.T\n",
    "    d = c.iloc[-1:, :].reset_index(drop=True)  # 0번 플레이어와 1번 플레이어의 행동 차이(각각, 전체)\n",
    "    d.columns = c.iloc[0, :] + \"_delta\"\n",
    "    e = c.iloc[1:2, :].reset_index(drop=True)  # 0번 플레이어\n",
    "    e.columns = c.iloc[0, :] + \"_0\"\n",
    "    f = c.iloc[2:3, :].reset_index(drop=True)  # 1번 플레이어\n",
    "    f.columns = c.iloc[0, :] + \"_1\"\n",
    "\n",
    "    df = pd.concat([a1, d, e, f, pd.Series(a2)], axis=1, ignore_index=True)\n",
    "    df['0_class'] = a3.iloc[:, 1].reset_index(drop=True)[0]\n",
    "    df['1_class'] = a3.iloc[:, 1].reset_index(drop=True)[1]\n",
    "    df.columns = list(a1.columns) + list(d.columns) + list(e.columns) + list(f.columns) + ['time', '0_class',\n",
    "                                                                                           '1_class']\n",
    "    return (df)\n",
    "\n",
    "# 전처리용 함수 2\n",
    "def make_data(x, type='train'):\n",
    "    print(\"=============================\")\n",
    "    print(\"start analysis\")\n",
    "    print(\"=============================\")\n",
    "    before_data = x.drop('event_contents', axis=1)\n",
    "    column_list = pd.Series(['Ability', 'AddToControlGroup', 'Camera', 'ControlGroup', 'GetControlGroup',\n",
    "                             'Right Click', 'Selection', 'SetControlGroup'])\n",
    "    cl = ['game_id', 'winner'] + list(column_list + '_delta') + list(column_list + '_0') + list(\n",
    "        column_list + '_1') + ['time', '0_class', '1_class']\n",
    "    data = pd.DataFrame({}, columns=cl)\n",
    "    start = x.game_id.min()\n",
    "    end = x.game_id.max()\n",
    "    for i in range(start, end + 1):\n",
    "        # for i in range(0, 10): # for test\n",
    "        if type == 'test':\n",
    "            df = make_feature(before_data, i, type)\n",
    "        else:\n",
    "            df = make_feature(before_data, i)\n",
    "        data = data.append(df, sort=False)\n",
    "        if i % 3000 == 0:\n",
    "            ii = i // 3000\n",
    "            ij = end // 3000\n",
    "            ik = ij - ii\n",
    "            print(\"=\" * ii + \" \" * ik + \"|\")\n",
    "            print(str(i) + \"/\" + str(end) + \" 작업 중..\")\n",
    "    print(\"=\" * ij + \"|\")\n",
    "    print(str(end) + \"/\" + str(end) + \" 작업 완료!\")\n",
    "    return (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# 행동 중 첫 카메라 좌표를 활용해 추가 feature 생성\n",
    "# KNN Clustering을 이용, 각 플레이어 첫 카메라 좌표를 조합해 스타팅 포인트 예측\n",
    "# 예측된 스타팅 포인트를 통해 경기에 사용된 map 생성(map 변수)\n",
    "# 사용된 스타팅 포인트의 좌표 간 거리 계산(distance 변수)\n",
    "############################################################\n",
    "def eda_starting(train, type='train'):\n",
    "    df_train = pd.DataFrame(train.game_id.unique(), columns=['game_id'])\n",
    "    df_train.index = df_train.game_id\n",
    "    df_train = df_train.drop(['game_id'], axis = 1)\n",
    "\n",
    "    # 처음 기록 된 카메라 좌표를 기록\n",
    "    df_train_p0 = train[(train.event=='Camera')&(train.player==0)]\n",
    "    df_train_p0 = df_train_p0[df_train_p0.shift(1).game_id!=df_train_p0.game_id]\n",
    "    if type=='train':\n",
    "        df_train_p0 = df_train_p0.iloc[:, [0,6]].rename({'event_contents':'player0_starting'}, axis = 1)\n",
    "    else:\n",
    "        df_train_p0 = df_train_p0.iloc[:, [0,5]].rename({'event_contents': 'player0_starting'}, axis=1)\n",
    "    df_train_p0.index = df_train_p0['game_id']\n",
    "    df_train_p0 = df_train_p0.drop(['game_id'], axis=1)\n",
    "    df_train = pd.merge(df_train, df_train_p0, on='game_id', how='left')\n",
    "    del df_train_p0\n",
    "\n",
    "    df_train_p1 = train[(train.event=='Camera')&(train.player==1)]\n",
    "    df_train_p1 = df_train_p1[df_train_p1.shift(1).game_id!=df_train_p1.game_id]\n",
    "    if type=='train':\n",
    "        df_train_p1 = df_train_p1.iloc[:, [0,6]].rename({'event_contents':'player1_starting'}, axis=1)\n",
    "    else:\n",
    "        df_train_p1 = df_train_p1.iloc[:, [0,5]].rename({'event_contents': 'player1_starting'}, axis=1)\n",
    "    df_train_p1.index = df_train_p1['game_id']\n",
    "    df_train_p1 = df_train_p1.drop(['game_id'], axis=1)\n",
    "    df_train = pd.merge(df_train, df_train_p1, on='game_id', how='left')\n",
    "    del df_train_p1\n",
    "    return(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 전처리 함수를 사용 첫 전처리 진행\n",
    "if __name__ == '__main__':\n",
    "    print(\"=============================\")\n",
    "    print(\"load data\")\n",
    "    print(\"=============================\")\n",
    "    data_path = \"\"\n",
    "    sample_submission = pd.read_csv(data_path + \"data/sample_submission.csv\")\n",
    "\n",
    "    if os.path.isfile(data_path + \"output/dacon3_data.csv\"):\n",
    "        print(\"already have train data\")\n",
    "        data = pd.read_csv(\"output/dacon3_data.csv\")\n",
    "    else:\n",
    "        train = pd.read_csv(data_path + \"data/train.csv\")\n",
    "        data = make_data(train)\n",
    "        data = data.fillna(0)\n",
    "        data.to_csv(\"output/dacon3_data.csv\", index=False)\n",
    "\n",
    "    if os.path.isfile(data_path + \"output/dacon3_test_data.csv\"):\n",
    "        print(\"already have test data\")\n",
    "        test_data = pd.read_csv(\"output/dacon3_test_data.csv\")\n",
    "    else:\n",
    "        test = pd.read_csv(data_path + \"data/test.csv\")\n",
    "        test_data = make_data(test, 'test')\n",
    "        test_data = test_data.fillna(0)\n",
    "        test_data.to_csv(\"output/dacon3_test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # KNN 사용, 거리 계산을 통해 두 개의 변수 추가\n",
    "    df1 = eda_starting(train)\n",
    "    df2 = eda_starting(test, type='test')\n",
    "\n",
    "    df_train = pd.concat([df1,df2], axis=0)\n",
    "\n",
    "\n",
    "    # x, y 값으로 분리\n",
    "    df_train['player0_starting'] = df_train.player0_starting.str.split('(').str[1].str.split(')').str[0]\n",
    "    split_xy = df_train.player0_starting.str.split(',')\n",
    "    df_train['player0_x'] = split_xy.str[0].astype('float')\n",
    "    df_train['player0_y'] = split_xy.str[1].astype('float')\n",
    "    del split_xy\n",
    "\n",
    "    df_train['player1_starting'] = df_train.player1_starting.str.split('(').str[1].str.split(')').str[0]\n",
    "    split_xy = df_train.player1_starting.str.split(',')\n",
    "    df_train['player1_x'] = split_xy.str[0].astype('float')\n",
    "    df_train['player1_y'] = split_xy.str[1].astype('float')\n",
    "    del split_xy\n",
    "\n",
    "    # 플레이어의 x,y 좌표를 하나로 모음\n",
    "    location_p0 = df_train.loc[:, ['player0_x', 'player0_y']]\n",
    "    location_p0 = location_p0.rename({'player0_x':'location_x', 'player0_y':'location_y'}, axis=1)\n",
    "    location_p1 = df_train.loc[:, ['player1_x', 'player1_y']]\n",
    "    location_p1 = location_p1.rename({'player1_x':'location_x', 'player1_y':'location_y'}, axis=1)\n",
    "    location_p1.index += location_p0.index[-1]+1\n",
    "    location = pd.concat([location_p0, location_p1])\n",
    "    location = location.dropna()\n",
    "    del location_p0, location_p1\n",
    "    df_train.player0_starting.value_counts().head(20)\n",
    "\n",
    "\n",
    "    kmeans_clst = KMeans(n_clusters=15).fit(location)\n",
    "    location['starting'] = kmeans_clst.labels_+1\n",
    "\n",
    "    for cluster in range(15):\n",
    "        point = location[location.starting==cluster+1]\n",
    "        loc = point.loc[:,['location_x', 'location_y']]\n",
    "        del point\n",
    "        loc['center_x'] = kmeans_clst.cluster_centers_[cluster][0]\n",
    "        loc['center_y'] = kmeans_clst.cluster_centers_[cluster][1]\n",
    "        distance = np.sqrt(np. square(loc.location_x - loc.center_x) + np.square(loc.location_y - loc.center_y))\n",
    "        location.loc[loc.index, 'distance'] = distance\n",
    "        del loc\n",
    "\n",
    "    idx = location[location.distance>5].index\n",
    "    location.loc[idx, 'starting'] = 0\n",
    "    del idx\n",
    "\n",
    "    df_train['player0_starting'] = location.loc[df_train.index, 'starting']\n",
    "    location.index -= (df_train.index[-1]+1)\n",
    "    df_train['player1_starting'] = location.loc[df_train.index, 'starting']\n",
    "    del location\n",
    "\n",
    "    df_train = df_train.fillna(0)\n",
    "\n",
    "    map_list = []\n",
    "    for point in range(1,16):\n",
    "        couple = df_train[df_train.player0_starting == point].player1_starting.value_counts()\n",
    "        if couple[couple.index[1]]<100:\n",
    "            map_list.append([point, couple.index[0], 999])\n",
    "        else:\n",
    "            map_list.append([point, couple.index[0], couple.index[1]])\n",
    "    map_list = np.sort(map_list, axis = 1)\n",
    "    map_list = np.unique(map_list, axis = 0)\n",
    "    for m in map_list:\n",
    "        idx = df_train[(df_train.player0_starting == 0) & (\n",
    "                    (df_train.player1_starting == m[0]) | (df_train.player1_starting == m[2]))].index\n",
    "        df_train.loc[idx, 'player0_starting'] = m[1]\n",
    "        del idx\n",
    "        idx = df_train[(df_train.player0_starting == 0) & (\n",
    "                    (df_train.player1_starting == m[1]) | (df_train.player1_starting == m[2]))].index\n",
    "        df_train.loc[idx, 'player0_starting'] = m[0]\n",
    "        del idx\n",
    "\n",
    "        idx = df_train[(df_train.player1_starting == 0) & (\n",
    "                    (df_train.player0_starting == m[0]) | (df_train.player0_starting == m[2]))].index\n",
    "        df_train.loc[idx, 'player1_starting'] = m[1]\n",
    "        del idx\n",
    "        idx = df_train[(df_train.player1_starting == 0) & (\n",
    "                    (df_train.player0_starting == m[1]) | (df_train.player0_starting == m[2]))].index\n",
    "        df_train.loc[idx, 'player1_starting'] = m[0]\n",
    "        del idx\n",
    "    df_train[(df_train.player0_starting == 0) | (df_train.player1_starting == 0)].head()\n",
    "\n",
    "\n",
    "\n",
    "    for map_num, m in enumerate(map_list):\n",
    "        idx = df_train[(df_train.player0_starting == m[0])|(df_train.player0_starting == m[1])|(df_train.player0_starting == m[2])].index\n",
    "        df_train.loc[idx, 'map'] = map_num\n",
    "    del idx, map_list\n",

    "    df_train.head()\n",
    "\n",
    "    df_train['distance_delta'] = np.sqrt(np. square(df_train.player0_x - df_train.player1_x)\n",
    "                                         + np.square(df_train.player0_y - df_train.player1_y))\n",
    "    df_train = df_train.loc[:,['map', 'distance_delta']]\n",
    "\n",
    "    clu_train = df_train.loc[train.game_id.unique(), :]\n",
    "    clu_test = df_train.loc[test.game_id.unique(), :]\n",
    "    clu_train.to_csv(\"clust_map_train.csv\")\n",
    "    clu_test.to_csv(\"clust_map_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ############################################################\n",
    "    # 최종 merge 및 인코딩, 변수 정리\n",
    "    # 이후 모델링 진행\n",
    "    ############################################################\n",
    "    \n",
    "    \n",
    "    # 의미 없다고 판단한 변수인 각 플레이어의 행동 카운트 제거\n",
    "    # 플레이어 간 카운트 차이가 유의하다고 판단.\n",
    "    # 지금 생각해보면 각 플레이어간 행동을 통해 플레이어를 클러스터링해서 고승률 유저를 구분하는건 어땠을까 싶음\n",
    "    data = pd.read_csv(\"output/dacon3_data.csv\")\n",
    "    clu = pd.read_csv(\"clust_map_train.csv\")\n",
    "    data2 = pd.merge(data, clu, how='inner', on='game_id')\n",
    "    data = data2.loc[:, ['game_id', 'winner', 'Ability_delta', 'AddToControlGroup_delta',\n",
    "           'Camera_delta', 'ControlGroup_delta', 'GetControlGroup_delta',\n",
    "           'Right Click_delta', 'Selection_delta', 'time', '0_class', '1_class', 'map', 'distance_delta']]\n",
    "    #data = data2.drop(['ALL_delta', 'ALL_0', 'ALL_1'], axis=1)\n",
    "\n",
    "    test_data = pd.read_csv(\"output/dacon3_test_data.csv\")\n",
    "    clu = pd.read_csv(\"clust_map_test.csv\")\n",
    "    data2 = pd.merge(test_data, clu, how='inner', on='game_id')\n",
    "    test_data = data2.loc[:, ['game_id', 'winner', 'Ability_delta', 'AddToControlGroup_delta',\n",
    "           'Camera_delta', 'ControlGroup_delta', 'GetControlGroup_delta',\n",
    "           'Right Click_delta', 'Selection_delta', 'time', '0_class', '1_class', 'map', 'distance_delta']]\n",
    "    #test_data = data2.drop(['ALL_delta', 'ALL_0', 'ALL_1'], axis=1)\n",
    "\n",
    "    # 스케일링은 크게 차이가 없어서 주석처리\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data[data.columns.difference(['game_id', 'winner', '0_class', '1_class','map'])])\n",
    "    data[data.columns.difference(['game_id', 'winner', '0_class', '1_class','map'])] = scaler.transform(\n",
    "        data[data.columns.difference(['game_id', 'winner', '0_class', '1_class','map'])])\n",
    "    test_data[data.columns.difference(['game_id', 'winner', '0_class', '1_class','map'])] = scaler.transform(\n",
    "        test_data[test_data.columns.difference(['game_id', 'winner', '0_class', '1_class','map'])])\n",
    "    \"\"\"\n",
    "    \n",
    "    # 종족간 승률을 보기 위해 상세 변수 생성, 필요한 자료 인코딩 진행\n",
    "    data['vs_class'] = data['0_class'] + data['1_class']\n",
    "    test_data['vs_class'] = test_data['0_class'] + test_data['1_class']\n",
    "\n",
    "    data['0_class'] = data['0_class'].str.replace('T','0').replace('Z','1').replace('P','2').astype('int')\n",
    "    data['1_class'] = data['1_class'].str.replace('T','0').replace('Z','1').replace('P','2').astype('int')\n",
    "    test_data['0_class'] = test_data['0_class'].str.replace('T','0').replace('Z','1').replace('P','2').astype('int')\n",
    "    test_data['1_class'] = test_data['1_class'].str.replace('T','0').replace('Z','1').replace('P','2').astype('int')\n",
    "    data = data.set_index('game_id')\n",
    "    test_data = test_data.set_index('game_id')\n",
    "    test = test_data.drop(['winner'], axis=1)\n",
    "    x_train = data[data.columns.difference(['winner'])]\n",
    "    y_train = data.winner\n",
    "\n",
    "    def onehot(data, col, col_name=None):\n",
    "        onehot = OneHotEncoder()\n",
    "        x = onehot.fit_transform(data[col].values.reshape(-1, 1)).toarray()\n",
    "        if col_name == None:\n",
    "            onehot_df = pd.DataFrame(x, columns=onehot.get_feature_names(), index=data.index)\n",
    "        else:\n",
    "            onehot_df = pd.DataFrame(x, columns=col_name, index=data.index)\n",
    "        x2 = pd.concat([data, onehot_df], axis=1)\n",
    "        x2 = x2.drop(col, axis=1)\n",
    "        return(x2)\n",
    "\n",
    "    # 사용하면 안 될 변수인 같은 종족전 제거(이것도 플레이어를 클러스터링했으면 사용할만 했을듯)\n",
    "    x_train = onehot(data=x_train, col='vs_class')\n",
    "    test = onehot(data=test, col='vs_class')\n",
    "    x_train = x_train.drop(['x0_PP', 'x0_TT', 'x0_ZZ',\n",
    "                            ], axis=1)\n",
    "    test = test.drop(['x0_PP', 'x0_TT', 'x0_ZZ',\n",
    "                      ], axis=1)\n",
    "    \n",
    "    # 굳이 원핫인코딩이 필요없는 자료는 레이블인코딩으로 진행, 교차곱은 의미없어서 주석처리\n",
    "    \"\"\"\n",
    "    map_col = ['map_0', 'map_1', 'map_2', 'map_3', 'map_4', 'map_5', 'map_6']\n",
    "    x_train = onehot(data=x_train,col='map',col_name=map_col)\n",
    "    test = onehot(data=test,col='map',col_name=map_col)\n",
    "    c0_col = ['0_0','0_1','0_2']\n",
    "    c1_col = ['1_0','1_1','1_2']\n",
    "    x_train = onehot(data=x_train,col='0_class',col_name=c0_col)\n",
    "    x_train = onehot(data=x_train,col='1_class',col_name=c1_col)\n",
    "    test = onehot(data=test,col='0_class',col_name=c0_col)\n",
    "    test = onehot(data=test,col='1_class',col_name=c1_col)\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def poly_col(a1, a2):\n",
    "        data2 = pd.DataFrame({}, index=a1.index)\n",
    "        len_a = len(a1.columns); len_b = len(a2.columns)\n",
    "        for j in range(len_b):\n",
    "            for i in range(len_a):\n",
    "                a3 = a1.iloc[:,range(len_a)[i]] * a2.iloc[:,range(len_b)[j]]\n",
    "                a3 = pd.DataFrame({a2.columns[j][-2:] + \"/\" + a1.columns[i] : a3})\n",
    "                data2 = pd.concat([data2,a3],axis=1)\n",
    "        return(data2)\n",
    "\n",
    "    a1 = x_train.iloc[:,-7:]\n",
    "    a2 = x_train.iloc[:,-13:-7]\n",
    "\n",
    "    x_train = pd.concat([x_train, poly_col(a1,a2)], axis=1)\n",
    "\n",
    "    x_train = x_train.drop(['x0_PT', 'x0_PZ', 'x0_TP', 'x0_TZ', 'x0_ZP', 'x0_ZT', 'map_0','map_1','map_2','map_3','map_4','map_5','map_6'], axis=1)\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    x_train = x_train.drop(['x0_0', 'x0_1', 'x0_2', 'x0_P',\n",
    "           'x0_T', 'x0_Z'], axis=1)\n",
    "    test = test.drop(['x0_0', 'x0_1', 'x0_2', 'x0_P',\n",
    "           'x0_T', 'x0_Z'], axis=1)\n",
    "    \"\"\"\n",
    "\n",
    "    # 데이터 자체가 10분 전후로 로그를 끊은거라 시간 컬럼이 무의미해서 제거\n",
    "    x_train2 = x_train\n",
    "    x_train = x_train.drop('time', axis=1)\n",
    "    test2 = test\n",
    "    test = test.drop('time', axis=1)\n",
    "\n",
    "    \"\"\"\n",
    "    x_train = x_train.drop(['x0_PT', 'x0_PZ', 'x0_TP', 'x0_TZ', 'x0_ZP', 'x0_ZT'], axis=1)\n",
    "    test = test.drop(['x0_PT', 'x0_PZ', 'x0_TP', 'x0_TZ', 'x0_ZP', 'x0_ZT'], axis=1)\n",
    "    \"\"\"\n",
    "\n",
    "    # 원핫인코딩 하지 않은 자료 카테고리화\n",
    "    x_train['0_class'] = x_train['0_class'].astype('category')\n",
    "    x_train['0_class'] = x_train['0_class'].astype('category')\n",
    "    x_train['map']= x_train['map'].astype('category')\n",
    "\n",
    "    test['0_class'] = test['0_class'].astype('category')\n",
    "    test['0_class'] = test['0_class'].astype('category')\n",
    "    test['map']= test['map'].astype('category')\n",
    "\n",
    "    # 간단하게 모델을 돌려봄\n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=120)\n",
    "    model = LGBMClassifier(n_estimators=1000, learning_rate=0.01, reg_lambda=1)\n",
    "    model.fit(x_tr, y_tr, early_stopping_rounds=200, eval_metric='auc', eval_set=[(x_val, y_val)])\n",
    "\n",
    "    pred_test = model.predict_proba(test)[:,1]\n",
    "    pd.DataFrame({'winner' : pred_test}, index=test.index).to_csv('output/time_sub.csv')\n",
    "    plot_importance(model)\n",
    "\n",
    "    # vs_class가 all보다 나음 / 0 ,1 로(no onehot) 한번 해보고 둘 중 하나만 쓰자\n",
    "    # map은 그냥 둠 0.6448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ############################################################\n",
    "    # 베이지안 옵티마이저를 통해 모델 튜닝 진행\n",
    "    # lightGBM 사용\n",
    "    ############################################################\n",
    "    def partial(func, *args, **keywords):\n",
    "        def newfunc(*fargs, **fkeywords):\n",
    "            newkeywords = keywords.copy()\n",
    "            newkeywords.update(fkeywords)\n",
    "            return func(*args, *fargs, **newkeywords)\n",
    "        newfunc.func = func\n",
    "        newfunc.args = args\n",
    "        newfunc.keywords = keywords\n",
    "        return newfunc\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score   # AUC 스코어 계산\n",
    "    from sklearn.model_selection import KFold   # K-fold CV\n",
    "    from bayes_opt import BayesianOptimization\n",
    "    import lightgbm as lgb\n",
    "\n",
    "    def lgb_cv(num_leaves, learning_rate, n_estimators, subsample, colsample_bytree, reg_alpha, reg_lambda, x_data=None,\n",
    "               y_data=None, n_splits=5, output='score'):\n",
    "        score = 0\n",
    "        kf = KFold(n_splits=n_splits)\n",
    "        models = []\n",
    "        for train_index, valid_index in kf.split(x_data):\n",
    "            x_train, y_train = x_data.iloc[train_index], y_data.iloc[train_index]\n",
    "            x_valid, y_valid = x_data.iloc[valid_index], y_data.iloc[valid_index]\n",
    "\n",
    "            model = lgb.LGBMClassifier(\n",
    "                num_leaves=int(num_leaves),\n",
    "                learning_rate=learning_rate,\n",
    "                n_estimators=int(n_estimators),\n",
    "                subsample=np.clip(subsample, 0, 1),\n",
    "                colsample_bytree=np.clip(colsample_bytree, 0, 1),\n",
    "                reg_alpha=reg_alpha,\n",
    "                reg_lambda=reg_lambda,\n",
    "            )\n",
    "\n",
    "            model.fit(x_train, y_train)\n",
    "            models.append(model)\n",
    "\n",
    "            pred = model.predict_proba(x_valid)[:, 1]\n",
    "            true = y_valid\n",
    "            score += roc_auc_score(true, pred) / n_splits\n",
    "\n",
    "        if output == 'score':\n",
    "            return score\n",
    "        if output == 'model':\n",
    "            return models\n",
    "\n",
    "    func_fixed = partial(lgb_cv, x_data=x_train, y_data=y_train, n_splits=5, output='score')\n",
    "    # 베이지안 최적화 범위 설정\n",
    "    lgbBO = BayesianOptimization(\n",
    "        func_fixed,\n",
    "        {\n",
    "            'num_leaves': (16, 1024),        # num_leaves,       범위(16~1024)\n",
    "            'learning_rate': (0.0001, 0.1),  # learning_rate,    범위(0.0001~0.1)\n",
    "            'n_estimators': (16, 1024),      # n_estimators,     범위(16~1024)\n",
    "            'subsample': (0, 1),             # subsample,        범위(0~1)\n",
    "            'colsample_bytree': (0, 1),      # colsample_bytree, 범위(0~1)\n",
    "            'reg_alpha': (0, 10),            # reg_alpha,        범위(0~10)\n",
    "            'reg_lambda': (0, 50),           # reg_lambda,       범위(0~50)\n",
    "        },\n",
    "        random_state=1025                    # 시드 고정\n",
    "    )\n",
    "    lgbBO.maximize(init_points=5, n_iter=30) # 처음 5회 랜덤 값으로 score 계산 후 30회 최적화\n",
    "\n",
    "    params = lgbBO.max['params']\n",
    "    models = lgb_cv(\n",
    "        params['num_leaves'],\n",
    "        params['learning_rate'],\n",
    "        params['n_estimators'],\n",
    "        params['subsample'],\n",
    "        params['colsample_bytree'],\n",
    "        params['reg_alpha'],\n",
    "        params['reg_lambda'],\n",
    "        x_data=x_train, y_data=y_train, n_splits=5, output='model')\n",
    "\n",
    "    preds = []\n",
    "    for model in models:\n",
    "        pred = model.predict_proba(test)[:, 1]\n",
    "        preds.append(pred)\n",
    "    pred = np.mean(preds, axis=0)\n",
    "\n",
    "    pd.DataFrame({'winner' : pred}, index=test.index).to_csv('submission_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "############################################################\n",
    "# 최종 0.628로 총 500명 중 전체 60등 정도지만 베이스라인으로 제시된 코드보다 낮음\n",
    "# 다음 대회부터는 eda부터 상세히 진행해봐야 할 듯\n",
    "# 아직 갈 길이 멀다\n",
    "############################################################\n",
    "############################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
