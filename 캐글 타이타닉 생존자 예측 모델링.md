# 캐글 타이타닉 생존자 예측 모델링
###### 이세욱

<pre><code>
import pandas as pd
import numpy as np
import seaborn as sns
tt = pd.read_csv('tt_train.csv', encoding='cp949')
</code></pre>



### EDA

컬럼은 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'로 구성되어 있다.



#### Pclass : 객실 등급

아마 일반실일 3은 거의 다 죽었다.
빈부에 따라 생존 차이가 나 보인다. 
1등급은 생존이 더 많다.

<pre><code>
pd.pivot_table(data=tt, columns='Survived', index='Pclass', values='PassengerId', aggfunc='count')
Survived    0    1
Pclass
1          80  136
2          97   87
3         372  119
</code></pre>



#### Embarked : 승선지역

S지역의 경우 사망자가 엄청나며, 애초에 사람의 수 자체도 많다는 걸 알 수 있다. 클래스간 차이가 난다.

<pre><code>
pd.pivot_table(data=tt, columns='Survived', index='Embarked', values='PassengerId', aggfunc='count')
Survived    0    1
Embarked
C          75   93
Q          47   30
S         427  217
</code></pre>



#### Sex : 성별

여자는 많이 살고, 남자는 대부분 죽는다. 유의미해보이는 변수이다.

<pre><code>
pd.pivot_table(data=tt, columns='Survived', index='Sex', values='PassengerId', aggfunc='count')
Survived    0    1
Sex
female     81  233
male      468  109
</code></pre>



####　Parch : 부모 동승자 수

혼자인 승객보다 아닌 승객이 생존이 조금 더 높아보이긴 하다. 3명 이상은 수가 적다.

<pre><code>
pd.pivot_table(data=tt, columns='Survived', index='Parch', values='PassengerId', aggfunc='count')
Survived      0      1
Parch
0         445.0  233.0
1          53.0   65.0
2          40.0   40.0
3           2.0    3.0
4           4.0    NaN
5           4.0    1.0
6           1.0    NaN
</code></pre>



#### SibSp : 형제자매 동승자 수

1명일 때는 생존자가 많고, 2명일 때는 비슷하다. 나머지는 사망자가 많아 보인다.

<pre><code>
pd.pivot_table(data=tt, columns='Survived', index='SibSp', values='PassengerId', aggfunc='count')
Survived      0      1
SibSp
0         398.0  210.0
1          97.0  112.0
2          15.0   13.0
3          12.0    4.0
4          15.0    3.0
5           5.0    NaN
8           7.0    NaN
</code></pre>



####　Fare : 요금

평균과 plot을 봤을 때 요금에 따라 생존 사망이 차이가 나 보인다.

<pre><code>
tt.loc[tt.Survived==0,'Fare'].mean()
Out[101]: 22.117886885245877
tt.loc[tt.Survived==1,'Fare'].mean()
Out[102]: 48.39540760233917
plt.plot(tt.loc[tt.Survived==0].Fare)
Out[98]: [<matplotlib.lines.Line2D at 0x2195f96fc50>]
plt.plot(tt.loc[tt.Survived==1].Fare)
Out[99]: [<matplotlib.lines.Line2D at 0x2195d6f4828>]
</code></pre>



#### Cabin

na가 너무 많다. 일단 배제하고 나중에 고민해보자.
다만 Cabin nan인 데이터와 값이 있는 데이터 간에 차이가 있어서 Cabin_adj 컬럼을 추가했다.

<code><pre>
tt['Cabin_adj'] = np.where(tt.Cabin.isnull(), 0, 1)
tt = tt.drop('Cabin', axis=1)
pd.pivot_table(data=tt, columns='Survived', index='Cabin_adj', values='PassengerId', aggfunc='count')
Survived     0    1
Cabin_adj
0          481  206
1           68  136
</code></pre>



####　Ticket

티켓이 다 제각각이라 의미를 찾기 어려웠다.



#### Age

na가 많았다.
na값을 변경해봐야 할 듯 하다.

<pre><code>
pd.pivot_table(data=tt, columns='Survived', index='Age', values='PassengerId', aggfunc='count')
plt.plot(tt.loc[tt.Survived==0].Age)
plt.plot(tt.loc[tt.Survived==1].Age)
</code></pre>



### 결측치 전처리

여기서부터는 test set의 결측치도 함께 제거하고자 한다.

<pre><code>
ts = pd.read_csv('tt_test.csv', encoding='cp949')
ts.describe() 
ts.loc[ts.Embarked.isnull()] 
# Age, Cabin, Fare에 결측치 존재(cabin은 지금 사용 안해서 패스함)
tt.describe() 
tt.loc[tt.Embarked.isnull()] 
# Age, Cabin, Embarked 에 결측치 존재(cabin은 지금 사용 안해서 패스함)
</code></pre>



#### Embarked

Embarked가 결측값인 사람 -> Ticket이 113572인 사람 -> Carbin이 B28인 사람 모두 동일해서 새 값을 지정하기 어려웠다.
cabin_adj 컬럼으로 Carbin이 존재하는 사람 대부분이 S클래스에서 탑승한 걸 알아내서 S클래스로 변경했다.

<pre><code>
tt.loc[tt.Embarked.isnull()] 
tt.loc[tt.Cabin_adj==1,'Embarked']
tt.loc[tt.Embarked.isnull(), 'Embarked']='S'
</code></pre>



#### Age

Age컬럼의 결측값은 170개 정도로 전체 891개의 상당한 부분을 차지해서 제거할 수 없었다.
나이에 따른 fare을 비교해봤으나 불규칙적이라 사용하기 힘들어보였다. Class 별로 구분해보았으나 이것 역시도 불규칙적이었다.

name의 중간에 mr, miss, mrs 등 나이를 유추할 만한 데이터를 찾아서 이 값들의 평균 값으로 결측치를 대체했다.

<pre><code>
tt.loc[tt.Age.isnull()]
tt['init'] = tt.Name.str.extract('([A-Za-z]+)\.')
pd.pivot_table(data=tt, index='init', columns='Survived', values='PassengerId', aggfunc='count')
Survived      0      1
init
Capt        1.0    NaN
Col         1.0    1.0
Countess    NaN    1.0
Don         1.0    NaN
Dr          4.0    3.0
Jonkheer    1.0    NaN
Lady        NaN    1.0
Major       1.0    1.0
Master     17.0   23.0
Miss       55.0  127.0
Mlle        NaN    2.0
Mme         NaN    1.0
Mr        436.0   81.0
Mrs        26.0   99.0
Ms          NaN    1.0
Rev         6.0    NaN
Sir         NaN    1.0
pd.pivot_table(data=tt, index='init', values='Age', aggfunc='mean')
                Age
init
Capt      70.000000
Col       58.000000
Countess  33.000000
Don       40.000000
Dr        42.000000
Jonkheer  38.000000
Lady      48.000000
Major     48.500000
Master     4.574167
Miss      21.773973
Mlle      24.000000
Mme       24.000000
Mr        32.368090
Mrs       35.898148
Ms        28.000000
Rev       43.166667
Sir       49.000000
ts['init'] = ts.Name.str.extract('([A-Za-z]+)\.')
pd.pivot_table(data=ts, index='init', values='PassengerId', aggfunc='count')
pd.pivot_table(data=ts, index='init', values='Age', aggfunc='mean')
</code></pre>


>capt, col, countess, don, jonkheer, lady, major, mlle, mme, ms, sir 은 너무 적은 숫자
>Dr = 7, Rev = 6도  다른 거에 비해 적어서 이것들은 다른거랑 합치거나 새로운 컬럼으로 바꾸어야 할듯
>Master = 40, Miss = 182, Mr = 517, Mrs 125은 유지한다.
>Lady(나이많은여자), Mme(부인), Countess(백작부인) 여성형은 mrs, //  Ms, Mlle(Mademoiselle는 miss와 비슷하대용) 는 miss 
>이어야하는데 mme가 생각보다 어려서 miss로 옮겼다. / test set의 dona는 여성형 명사로 39세라 비슷한 mrs로 옮겼다.
>Dr, Major, Sir, Capt,Rev(목사인데 남성이라 가정), Col(Colonel : 대령), Don(도날드), Jonkheer(네덜란드 남성형 명사)는 남성형인데 mr의 평균 나이인 32보다 너무 커서
>Gen 으로 변경했다.
>남성 자체가 master 어린아이를 제외하면 대부분 사망했어서 나누지 않아도 비슷할수도 있을 듯 하다.

<pre><code>
tt['init'].replace(['Lady','Countess','Mme','Ms','Mlle','Dr','Major','Sir','Capt','Don','Jonkheer','Rev','Col'],
['Mrs','Mrs','Miss','Miss','Miss','Gen','Gen','Gen','Gen','Gen','Gen','Gen','Gen'],inplace=True)
pd.pivot_table(data=tt, index='init', values='Age', aggfunc='mean')
              Age
init
Gen     46.050000
Master   4.574167
Miss    21.860000
Mr      32.368090
Mrs     35.981818
a1 = pd.pivot_table(data=tt, index='init', values='Age', aggfunc='mean')
a2 = tt.loc[tt.Age.isnull()]
m1 = pd.merge(a2,a1, left_on='init', right_index=True)
tt.loc[tt.Age.isnull(), 'Age'] = m1.Age_y
ts['init'].replace(['Col','Dona','Dr','Ms','Rev'], ['Gen','Mrs','Gen','Miss','Gen'],inplace=True)
pd.pivot_table(data=ts, index='init', values='Age', aggfunc='mean')
b1 = pd.pivot_table(data=ts, index='init', values='Age', aggfunc='mean')
b2 = ts.loc[ts.Age.isnull()]
m2 = pd.merge(b2,b1, left_on='init', right_index=True)
ts.loc[ts.Age.isnull(), 'Age'] = m2.Age_y
</code></pre>



#### Fare

test set의 fare가 하나 없어서 Class 3의 평균값을 집어 넣었다.

<pre><code>
ts.loc[ts.Fare.isnull()]
pd.pivot_table(data=ts, index=['Pclass','Sex'], values='Fare', aggfunc='mean') 
pd.pivot_table(data=ts, index='Pclass', values='Fare', aggfunc='mean') # 12.459678
ts.loc[ts.Fare.isnull(), 'Fare'] = 12.459678
</code></pre>



#### family 생성

기존의 Parch, Sibsp는 인원수가 적어서 애매했는데 이를 합치기로 하였다.
1명, 2명, 3명일 때 생존이 높고 나머지는 사망이 많다.

<pre><code>
tt['Family'] = tt.Parch + tt.SibSp
pd.pivot_table(data=tt, columns='Survived', index='Family', values='PassengerId', aggfunc='count')
</code></pre>



### 상관관계 분석

현재 사용할 컬럼인 Survived(Y) ~ Pclass, Sex, Age, SibSp, Parch, Fare, Embarked, Cabin_adj, Family
간의 상관관계를 분석해보았다.

Survived~Pclass, Survived~Sex, 두 가지가 Y에 영향력이 있어 보인다.
변수간에는 Pclass~Fare 정도가 영향이 있어 보인다. 
이외에는 크게 영향이 없어보이며 pclass fare도 크게 상관관계가 느껴지는 것은 아니기에 그냥 사용하기로 했다.

<code><pre>
tt.Embarked = tt.Embarked.map({'C': 0, 'Q': 1, 'S': 2})
tt.init = tt.init.map({'Master': 0, 'Miss': 1, 'Mr': 2, 'Mrs': 3, 'Gen': 4})
tt.Sex= tt.Sex.map({'female': 0, 'male': 1})
corr = tt.loc[:,['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Family']].corr(method = 'pearson')
import seaborn as sns 
colormap = plt.cm.RdBu
plt.figure(figsize=(10, 10))
sns.heatmap(corr, linewidths=0.1, vmax=1.0,
           square=True, cmap=colormap, linecolor='white', annot=True)
tt_final = tt.loc[:,['PassengerId', 'Survived', 'Pclass', 'Sex', 'Age', 'SibSp',
       'Parch', 'Fare', 'Embarked', 'init', 'Family', 'Cabin_adj']].set_index('PassengerId')
tt_final.to_csv('tt_final.csv', encoding='cp949')
ts['Cabin_adj'] = np.where(ts.Cabin.isnull(), 0, 1)
ts['Family'] = ts.Parch + ts.SibSp
ts_final = ts.loc[:,['PassengerId', 'Pclass', 'Sex', 'Age', 'SibSp',
       'Parch', 'Fare', 'Embarked', 'init', 'Family', 'Cabin_adj']].set_index('PassengerId')
ts_final.to_csv('ts_final.csv', encoding='cp949')
</code></pre>



### 모델링

그래디언트부스팅 분류모델을 통해 생존자를 예측해 보았다.
랜덤포레스트도 사용했지만 GB가 더 좋은 결과를 만들었다.
최종적으로는 Family, Cabin_adj를 제외하고 
그리드서치(+ 교차검증)을 통해 최적의

<pre><code>
tt = pd.read_csv('tt_final.csv', encoding='cp949')
from sklearn.preprocessing import OneHotEncoder
x = OneHotEncoder().fit_transform(tt['Embarked'].values.reshape(-1,1)).toarray()
onehot_df = pd.DataFrame(x, columns = ["Emb_"+str(int(i)) for i in range(x.shape[1])])
tt = pd.concat([tt, onehot_df], axis=1)
x = OneHotEncoder().fit_transform(tt['init'].values.reshape(-1,1)).toarray()
onehot_df = pd.DataFrame(x, columns = ["init_"+str(int(i)) for i in range(x.shape[1])])
tt = pd.concat([tt, onehot_df], axis=1)
final2 = tt.drop(['Embarked','init'], axis=1)
final2 = final2.set_index('PassengerId')
x = OneHotEncoder().fit_transform(tt['Pclass'].values.reshape(-1,1)).toarray()
onehot_df = pd.DataFrame(x, columns = ["Pclass_"+str(int(i)) for i in range(x.shape[1])])
tt = pd.concat([tt, onehot_df], axis=1)
final = tt.drop(['Embarked','Pclass','init'], axis=1)
final = final.set_index('PassengerId')
X_train6 = final.iloc[:,1:].drop(['Parch','SibSp','Family','Cabin_adj'], axis=1)
y_train = final.iloc[:,0]
from sklearn.preprocessing import MinMaxScaler
scaler6 = MinMaxScaler()
X_train6 = scaler6.fit_transform(X_train6)
tt2 = pd.read_csv('ts_final.csv', encoding='cp949')
tt2.Embarked = tt2.Embarked.map({'C': 0, 'Q': 1, 'S': 2})
tt2.init = tt2.init.map({'Master': 0, 'Miss': 1, 'Mr': 2, 'Mrs': 3, 'Gen': 4})
tt2.Sex= tt2.Sex.map({'female': 0, 'male': 1})
from sklearn.preprocessing import OneHotEncoder
x = OneHotEncoder().fit_transform(tt2['Embarked'].values.reshape(-1,1)).toarray()
onehot_df = pd.DataFrame(x, columns = ["Emb_"+str(int(i)) for i in range(x.shape[1])])
tt2 = pd.concat([tt2, onehot_df], axis=1)
x = OneHotEncoder().fit_transform(tt2['init'].values.reshape(-1,1)).toarray()
onehot_df = pd.DataFrame(x, columns = ["init_"+str(int(i)) for i in range(x.shape[1])])
tt2 = pd.concat([tt2, onehot_df], axis=1)
x = OneHotEncoder().fit_transform(tt2['Pclass'].values.reshape(-1,1)).toarray()
onehot_df = pd.DataFrame(x, columns = ["Pclass_"+str(int(i)) for i in range(x.shape[1])])
tt2 = pd.concat([tt2, onehot_df], axis=1)
finaltest = tt2.drop(['Embarked','Pclass','init'], axis=1)
finaltest = finaltest.set_index('PassengerId')
ft6 = finaltest.drop(['Parch','SibSp','Family','Cabin_adj'], axis=1)
ft6 = scaler6.transform(ft6)
</code></pre>

<pre><code>
from sklearn.ensemble import RandomForestClassifier # RF 사용했지만 GB가 더 나은 결과를 보여줌
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingClassifier
param_grid = {'max_depth' : [1,2,3,4,5,6,7], 'learning_rate' : [1,0.1,0.05,0.01,0.005,0.001], 'random_state' : [2]}
grid_search = GridSearchCV( GradientBoostingClassifier(), param_grid, cv=5)
grid_search.fit(X_train6, y_train)
grid_search.best_params_ # 최적 매개변수
#  {'learning_rate': 0.1, 'max_depth': 4, 'random_state': 2}
grid_search.best_score_ # 점수
# 0.8406285072951739
grid_search.best_estimator_ # 최적 매개변수를 넣은 best_estimator
</code></pre>

<pre><code>
test_score = pd.DataFrame({'PassengerId' : finaltest.index, 'Survived' : grid_search.best_estimator_.predict(ft6)})
test_score.to_csv('Predict_6.csv', encoding='cp949', index=False)
</code></pre>



### 결과

Train Set 84%, Test Set 78%로
캐글코리아 타이타닉 예측 대회에서는 43등대의 점수와 같았다.(전체 353)
기존 대회에선 6000/12000 으로 중간 정도의 성적이 나왔다.